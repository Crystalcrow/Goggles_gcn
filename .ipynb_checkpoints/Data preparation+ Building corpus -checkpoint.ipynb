{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_path = \"C:/Users/adity/OneDrive/Desktop/nlu_course/Goggles/\"\n",
    "\n",
    "train_csv = common_path + 'train.csv'\n",
    "test_csv = common_path + 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zone_text_converter(val):\n",
    "    val = re.sub(r'([^\\s\\w.,]|_)+', '', val).lower()\n",
    "    val = re.sub(r',+', '.', val)\n",
    "    val = re.sub(r'\\d+', '', val)\n",
    "    val = re.sub(r' +',' ',val)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def field_converter(val):\n",
    "    return val.lower().replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s, word_remove_list=None):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "#     print(stop_words)\n",
    "    x = s.split()\n",
    "    filtered_words = []\n",
    "    for w in x:\n",
    "        if w not in stop_words and len(w)>1:\n",
    "            filtered_words.append(w)\n",
    "    return((' ').join(filtered_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_csv, index_col=0, converters={'zone_text': zone_text_converter, 'subject': field_converter,\n",
    "                                           'grade': field_converter, 'syllabus': field_converter})\n",
    "test_df = pd.read_csv(test_csv, index_col=0, converters={'zone_text': zone_text_converter, 'subject': field_converter,\n",
    "                                           'grade': field_converter, 'syllabus': field_converter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df.category_type=='lesson']\n",
    "test_df = test_df[test_df.category_type=='lesson']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train=[]\n",
    "text_train_list = list(train_df['zone_text'])\n",
    "con_train_list = list(train_df['annotation_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['before we solve problems involving direction of current . direction of magnetic field and the direction of force by using fleming s left hand rule . we should keep the following points in mind i by convention . the direction of flow of positive charges is taken to be the direction of flow of current . so . the direction in which the positively charged particles such as protons or alpha particles . etc . . move will be the direction of electric current . ii the direction of electric current is . however . taken to be opposite to the direction of flow of negative charges such as electrons . so . if we are given the direction of flow of electrons . then the direction of electric current will be taken as opposite to the direction of flow of electrons . iii the direction of deflection of a current carrying conductor or a stream of positively charged particles or a stream of negatively charged particles like electrons tells us the direction of force acting on it . let us solve some problems now .',\n",
       " 'sources of geothermal energy . the geothermal energy is harnessed as follows ',\n",
       " 'wind energy definition flow of air is called wind . it possesses enormous energy . as the energy is due to motion of air during its flow . the energy is kinetic . source of wind energy solar energy heats the entire earth but the heating is not uniform . the heating is more intense near the equator than in the polar region . this makes air in the equator region more hot and light . it rises up and its space is filled with the cooler air from polar region . in this way air flows from colder region at high pressure to hotter region at low pressure . the flow of air from one place to other constitutes wind . the smooth flow of air is disturbed continuously by rotation of the earth as well as by the local conditions . due to these interacting factor the wind speed may vary from km h to about km h gentle breeze to very high speed of about km h of a storm tornado .']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[730, 764, 753]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_train_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2074"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text_train_list)):\n",
    "    s = 'train'\n",
    "    L_train.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2074"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train',\n",
       " 'train',\n",
       " 'train',\n",
       " 'train',\n",
       " 'train',\n",
       " 'train',\n",
       " 'train',\n",
       " 'train',\n",
       " 'train',\n",
       " 'train']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=[]\n",
    "text_l = list(test_df['zone_text'])\n",
    "ann_l = list(test_df['annotation_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['let us perform an experiment to verify this fact . take a straight copper rod . suspend it horizontally by means of two connecting wires between the poles of a strong horseshoe magnet as shown in figure . a and b . if a current is now passed in the rod as shown in figure . a . you will observe that the rod gets displaced . this displacement is caused by the force acting on the current carrying rod . in accordance with fleming s left hand rule . the magnet exerts a force on the rod directed upwards . with the result that the rod will get deflected upwards . now . reverse the direction of current or interchange the poles of the magnet as shown in figure . b . you will observe that the rod is now displaced downwards i . e . . deflection of the rod has reversed . this indicates that the direction of the force acting on the rod has reversed .',\n",
       " 'equivalent resistance in parallel connection figure . a shows three resistors of resistances r . r and r connected in parallel across the points a and b . the cell connected across these two points maintains a potential difference v across each resistor . the current through the cell is i . it gets divided at a into three parts iſ . in and iz . which flow through r . r and r respectively . reg fig . . let us replace the combination of resistors by an equivalent resistor reg such that the current i in the circuit does not change figure . a . the equivalent resistance is given by ohm s law as req v i . thus . rea the currents iż . iż and iz through the resistors r . r and r respectively are given by ohm s law as since the resistors are in parallel . i iz iz iz .',\n",
       " 'table . symbols of electrical elements used in electrical circuits . m lamp or load connecting wire wires crossing wires connected together e a cell battery ac source fixed resistance rheostat g hey switch off voltmeter ammeter galvanometer ground earth o key open switch closed key closed variable resistance . lamp or load it is an arrangement across which the output is obtained .']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_l[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[726, 560, 552]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_l[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ann_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text_l)):\n",
    "    s = 'test'\n",
    "    L.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = text_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.extend(text_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2582"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'let us perform an experiment to verify this fact . take a straight copper rod . suspend it horizontally by means of two connecting wires between the poles of a strong horseshoe magnet as shown in figure . a and b . if a current is now passed in the rod as shown in figure . a . you will observe that the rod gets displaced . this displacement is caused by the force acting on the current carrying rod . in accordance with fleming s left hand rule . the magnet exerts a force on the rod directed upwards . with the result that the rod will get deflected upwards . now . reverse the direction of current or interchange the poles of the magnet as shown in figure . b . you will observe that the rod is now displaced downwards i . e . . deflection of the rod has reversed . this indicates that the direction of the force acting on the rod has reversed .'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[2074]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2582"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = con_train_list\n",
    "labels.extend(ann_l)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "726"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[2074]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['730', '764', '753']\n",
      "726\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "labels = [str(i) for i in labels]\n",
    "print(labels[:3])\n",
    "print(labels[2074])\n",
    "print(type(labels[2074]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2582"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_or_test_list = L_train\n",
    "train_or_test_list.extend(L)\n",
    "len(train_or_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Goggles'\n",
    "# sentences = ['Would you like a plain sweater or something else?​', \n",
    "#              'Great. We have some very nice wool slacks over here. Would you like to take a look?',\n",
    "#             'This tutorial is intended to be a gentle introduction to argparse, the recommended command-line parsing module in the Python standard library.',\n",
    "#             'Let’s show the sort of functionality that we are going to explore in this introductory tutorial by making use of the ls command:',\n",
    "#             'The ls command is useful when run without any options at all. It defaults to displaying the contents of the current directory.',\n",
    "#             'If we want beyond what it provides by default, we tell it a bit more. In this case, we want it to display a different directory',\n",
    "#             'That’s a snippet of the help text.']\n",
    "# labels = ['Yes' , 'No', 'maybe', 'No','Yes','maybe','Yes' ]\n",
    "# train_or_test_list = ['train', 'test','train','train','test','test','train']\n",
    "\n",
    "\n",
    "meta_data_list = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    meta = str(i) + '\\t' + train_or_test_list[i] + '\\t' + labels[i]\n",
    "    meta_data_list.append(meta)\n",
    "\n",
    "meta_data_str = '\\n'.join(meta_data_list)\n",
    "\n",
    "f = open('data/' + dataset_name + '.txt', 'w')\n",
    "f.write(meta_data_str)\n",
    "f.close()\n",
    "\n",
    "corpus_str = '\\n'.join(sentences)\n",
    "\n",
    "f = open('data/corpus/' + dataset_name + '.txt', 'w', encoding=\"utf-8\")\n",
    "f.write(corpus_str)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adity\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'these', 'ours', 'into', 'weren', 'through', 'm', 'aren', 'his', 'same', 'for', 're', 'haven', 'themselves', 'am', 'each', \"hadn't\", \"shan't\", 'only', 'both', 'few', 'this', 'then', 'is', 'yourself', 'll', \"wouldn't\", 'below', 'me', 'they', 'on', \"couldn't\", \"didn't\", 'couldn', 'why', 'needn', 'mustn', 'up', 'over', 'whom', 'once', 'your', \"that'll\", 'my', 'by', 'further', 'some', 'here', 'more', 'when', 'all', 'hadn', 'myself', 'ourselves', 'doing', 'most', 'with', 'that', 'was', 'against', 'so', 'y', 'mightn', \"mightn't\", 'ma', 'doesn', 'an', \"shouldn't\", 'just', \"you'd\", 'but', \"isn't\", 'who', 'were', 'as', 'there', 'herself', 'than', 'do', \"wasn't\", 'any', 'down', 'yourselves', 'does', 'them', 'too', 'its', 'had', 'don', 'are', 'how', 'didn', 'above', 'no', 'if', 'shouldn', 'now', 'being', 'to', 'shan', 'such', \"aren't\", 'those', 'off', \"weren't\", 'in', \"needn't\", 'while', 'under', 'hasn', 'very', 'where', 'himself', \"you've\", 'their', 'she', 'did', 'he', 'should', 'itself', 'you', \"hasn't\", 'have', \"don't\", 'or', 'we', 'isn', 'i', 'own', 'a', 'which', 'has', 'the', 'will', 'be', \"mustn't\", 'at', 've', \"haven't\", \"it's\", 'having', 'about', 'yours', 'it', 'can', 'ain', 'wasn', 's', 'her', \"doesn't\", 'theirs', 'other', 'and', 'during', 'not', 'him', \"you'll\", 'again', \"won't\", 'before', 'hers', 'wouldn', 'd', \"she's\", \"you're\", 'our', 'o', 'because', 'of', 'out', 'nor', 'won', \"should've\", 'after', 'what', 'until', 't', 'between', 'from', 'been'}\n",
      "min_len : 2\n",
      "max_len : 555\n",
      "average_len : 42.91828040278853\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from utils import clean_str, loadWord2Vec\n",
    "import sys\n",
    "\n",
    "# if len(sys.argv) != 2:\n",
    "# \tsys.exit(\"Use: python remove_words.py <dataset>\")\n",
    "\n",
    "# datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\n",
    "# dataset = sys.argv[1]\n",
    "\n",
    "# if dataset not in datasets:\n",
    "# \tsys.exit(\"wrong dataset name\")\n",
    "\n",
    "dataset = 'Goggles'\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "\n",
    "# Read Word Vectors\n",
    "# word_vector_file = 'data/glove.6B/glove.6B.200d.txt'\n",
    "# vocab, embd, word_vector_map = loadWord2Vec(word_vector_file)\n",
    "# word_embeddings_dim = len(embd[0])\n",
    "# dataset = '20ng'\n",
    "\n",
    "doc_content_list = []\n",
    "f = open('data/corpus/' + dataset + '.txt', 'rb')\n",
    "# f = open('data/wiki_long_abstracts_en_text.txt', 'r')\n",
    "for line in f.readlines():\n",
    "    doc_content_list.append(line.strip().decode('latin1'))\n",
    "f.close()\n",
    "\n",
    "\n",
    "word_freq = {}  # to remove rare words\n",
    "\n",
    "for doc_content in doc_content_list:\n",
    "    temp = clean_str(doc_content)\n",
    "    words = temp.split()\n",
    "    for word in words:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "clean_docs = []\n",
    "for doc_content in doc_content_list:\n",
    "    temp = clean_str(doc_content)\n",
    "    words = temp.split()\n",
    "    doc_words = []\n",
    "    for word in words:\n",
    "        # word not in stop_words and word_freq[word] >= 5\n",
    "        if dataset == 'mr':\n",
    "            doc_words.append(word)\n",
    "        elif word not in stop_words and word_freq[word] >= 1:\n",
    "            doc_words.append(word)\n",
    "\n",
    "    doc_str = ' '.join(doc_words).strip()\n",
    "    #if doc_str == '':\n",
    "        #doc_str = temp\n",
    "    clean_docs.append(doc_str)\n",
    "\n",
    "clean_corpus_str = '\\n'.join(clean_docs)\n",
    "\n",
    "f = open('data/corpus/' + dataset + '.clean.txt', 'w')\n",
    "#f = open('data/wiki_long_abstracts_en_text.clean.txt', 'w')\n",
    "f.write(clean_corpus_str)\n",
    "f.close()\n",
    "\n",
    "#dataset = '20ng'\n",
    "min_len = 10000\n",
    "aver_len = 0\n",
    "max_len = 0 \n",
    "\n",
    "f = open('data/corpus/' + dataset + '.clean.txt', 'r')\n",
    "#f = open('data/wiki_long_abstracts_en_text.txt', 'r')\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    temp = line.split()\n",
    "    aver_len = aver_len + len(temp)\n",
    "    if len(temp) < min_len:\n",
    "        min_len = len(temp)\n",
    "    if len(temp) > max_len:\n",
    "        max_len = len(temp)\n",
    "f.close()\n",
    "aver_len = 1.0 * aver_len / len(lines)\n",
    "print('min_len : ' + str(min_len))\n",
    "print('max_len : ' + str(max_len))\n",
    "print('average_len : ' + str(aver_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3, 6]\n",
      "[1, 4, 5]\n",
      "[0, 2, 6, 3, 4, 1, 5]\n",
      "7\n",
      "[[1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]]\n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "(4, 100) (4, 3) (3, 100) (3, 3) (62, 100) (62, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from utils import loadWord2Vec, clean_str\n",
    "from math import log\n",
    "from sklearn import svm\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# if len(sys.argv) != 2:\n",
    "# \tsys.exit(\"Use: python build_graph.py <dataset>\")\n",
    "\n",
    "# datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\n",
    "# # build corpus\n",
    "# dataset = sys.argv[1]\n",
    "\n",
    "# if dataset not in datasets:\n",
    "# \tsys.exit(\"wrong dataset name\")\n",
    "\n",
    "# Read Word Vectors\n",
    "# word_vector_file = 'data/glove.6B/glove.6B.300d.txt'\n",
    "# word_vector_file = 'data/corpus/' + dataset + '_word_vectors.txt'\n",
    "#_, embd, word_vector_map = loadWord2Vec(word_vector_file)\n",
    "# word_embeddings_dim = len(embd[0])\n",
    "\n",
    "word_embeddings_dim = 100\n",
    "word_vector_map = {}\n",
    "\n",
    "# shulffing\n",
    "doc_name_list = []\n",
    "doc_train_list = []\n",
    "doc_test_list = []\n",
    "\n",
    "f = open('data/' + dataset + '.txt', 'r')\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    doc_name_list.append(line.strip())\n",
    "    temp = line.split(\"\\t\")\n",
    "    if temp[1].find('test') != -1:\n",
    "        doc_test_list.append(line.strip())\n",
    "    elif temp[1].find('train') != -1:\n",
    "        doc_train_list.append(line.strip())\n",
    "f.close()\n",
    "# print(doc_train_list)\n",
    "# print(doc_test_list)\n",
    "\n",
    "doc_content_list = []\n",
    "f = open('data/corpus/' + dataset + '.clean.txt', 'r')\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    doc_content_list.append(line.strip())\n",
    "f.close()\n",
    "# print(doc_content_list)\n",
    "\n",
    "train_ids = []\n",
    "for train_name in doc_train_list:\n",
    "    train_id = doc_name_list.index(train_name)\n",
    "    train_ids.append(train_id)\n",
    "print(train_ids)\n",
    "random.shuffle(train_ids)\n",
    "\n",
    "# partial labeled data\n",
    "#train_ids = train_ids[:int(0.2 * len(train_ids))]\n",
    "\n",
    "train_ids_str = '\\n'.join(str(index) for index in train_ids)\n",
    "f = open('data/' + dataset + '.train.index', 'w')\n",
    "f.write(train_ids_str)\n",
    "f.close()\n",
    "\n",
    "test_ids = []\n",
    "for test_name in doc_test_list:\n",
    "    test_id = doc_name_list.index(test_name)\n",
    "    test_ids.append(test_id)\n",
    "print(test_ids)\n",
    "random.shuffle(test_ids)\n",
    "\n",
    "test_ids_str = '\\n'.join(str(index) for index in test_ids)\n",
    "f = open('data/' + dataset + '.test.index', 'w')\n",
    "f.write(test_ids_str)\n",
    "f.close()\n",
    "\n",
    "ids = train_ids + test_ids\n",
    "print(ids)\n",
    "print(len(ids))\n",
    "\n",
    "shuffle_doc_name_list = []\n",
    "shuffle_doc_words_list = []\n",
    "for id in ids:\n",
    "    shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
    "    shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
    "shuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\n",
    "shuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n",
    "\n",
    "f = open('data/' + dataset + '_shuffle.txt', 'w')\n",
    "f.write(shuffle_doc_name_str)\n",
    "f.close()\n",
    "\n",
    "f = open('data/corpus/' + dataset + '_shuffle.txt', 'w')\n",
    "f.write(shuffle_doc_words_str)\n",
    "f.close()\n",
    "\n",
    "# build vocab\n",
    "word_freq = {}\n",
    "word_set = set()\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_doc_list = {}\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    appeared = set()\n",
    "    for word in words:\n",
    "        if word in appeared:\n",
    "            continue\n",
    "        if word in word_doc_list:\n",
    "            doc_list = word_doc_list[word]\n",
    "            doc_list.append(i)\n",
    "            word_doc_list[word] = doc_list\n",
    "        else:\n",
    "            word_doc_list[word] = [i]\n",
    "        appeared.add(word)\n",
    "\n",
    "word_doc_freq = {}\n",
    "for word, doc_list in word_doc_list.items():\n",
    "    word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "word_id_map = {}\n",
    "for i in range(vocab_size):\n",
    "    word_id_map[vocab[i]] = i\n",
    "\n",
    "vocab_str = '\\n'.join(vocab)\n",
    "\n",
    "f = open('data/corpus/' + dataset + '_vocab.txt', 'w')\n",
    "f.write(vocab_str)\n",
    "f.close()\n",
    "\n",
    "'''\n",
    "Word definitions begin\n",
    "'''\n",
    "'''\n",
    "definitions = []\n",
    "\n",
    "for word in vocab:\n",
    "    word = word.strip()\n",
    "    synsets = wn.synsets(clean_str(word))\n",
    "    word_defs = []\n",
    "    for synset in synsets:\n",
    "        syn_def = synset.definition()\n",
    "        word_defs.append(syn_def)\n",
    "    word_des = ' '.join(word_defs)\n",
    "    if word_des == '':\n",
    "        word_des = '<PAD>'\n",
    "    definitions.append(word_des)\n",
    "\n",
    "string = '\\n'.join(definitions)\n",
    "\n",
    "\n",
    "f = open('data/corpus/' + dataset + '_vocab_def.txt', 'w')\n",
    "f.write(string)\n",
    "f.close()\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix = tfidf_vec.fit_transform(definitions)\n",
    "tfidf_matrix_array = tfidf_matrix.toarray()\n",
    "print(tfidf_matrix_array[0], len(tfidf_matrix_array[0]))\n",
    "\n",
    "word_vectors = []\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab[i]\n",
    "    vector = tfidf_matrix_array[i]\n",
    "    str_vector = []\n",
    "    for j in range(len(vector)):\n",
    "        str_vector.append(str(vector[j]))\n",
    "    temp = ' '.join(str_vector)\n",
    "    word_vector = word + ' ' + temp\n",
    "    word_vectors.append(word_vector)\n",
    "\n",
    "string = '\\n'.join(word_vectors)\n",
    "\n",
    "f = open('data/corpus/' + dataset + '_word_vectors.txt', 'w')\n",
    "f.write(string)\n",
    "f.close()\n",
    "\n",
    "word_vector_file = 'data/corpus/' + dataset + '_word_vectors.txt'\n",
    "_, embd, word_vector_map = loadWord2Vec(word_vector_file)\n",
    "word_embeddings_dim = len(embd[0])\n",
    "'''\n",
    "\n",
    "'''\n",
    "Word definitions end\n",
    "'''\n",
    "\n",
    "# label list\n",
    "label_set = set()\n",
    "for doc_meta in shuffle_doc_name_list:\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label_set.add(temp[2])\n",
    "label_list = list(label_set)\n",
    "\n",
    "label_list_str = '\\n'.join(label_list)\n",
    "f = open('data/corpus/' + dataset + '_labels.txt', 'w')\n",
    "f.write(label_list_str)\n",
    "f.close()\n",
    "\n",
    "# x: feature vectors of training docs, no initial features\n",
    "# slect 90% training set\n",
    "train_size = len(train_ids)\n",
    "val_size = int(0.1 * train_size)\n",
    "real_train_size = train_size - val_size  # - int(0.5 * train_size)\n",
    "# different training rates\n",
    "\n",
    "real_train_doc_names = shuffle_doc_name_list[:real_train_size]\n",
    "real_train_doc_names_str = '\\n'.join(real_train_doc_names)\n",
    "\n",
    "f = open('data/' + dataset + '.real_train.name', 'w')\n",
    "f.write(real_train_doc_names_str)\n",
    "f.close()\n",
    "\n",
    "row_x = []\n",
    "col_x = []\n",
    "data_x = []\n",
    "for i in range(real_train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            # print(doc_vec)\n",
    "            # print(np.array(word_vector))\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_x.append(i)\n",
    "        col_x.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_x.append(doc_vec[j] / doc_len)  # doc_vec[j]/ doc_len\n",
    "\n",
    "# x = sp.csr_matrix((real_train_size, word_embeddings_dim), dtype=np.float32)\n",
    "x = sp.csr_matrix((data_x, (row_x, col_x)), shape=(\n",
    "    real_train_size, word_embeddings_dim))\n",
    "\n",
    "y = []\n",
    "for i in range(real_train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    y.append(one_hot)\n",
    "y = np.array(y)\n",
    "print(y)\n",
    "\n",
    "# tx: feature vectors of test docs, no initial features\n",
    "test_size = len(test_ids)\n",
    "\n",
    "row_tx = []\n",
    "col_tx = []\n",
    "data_tx = []\n",
    "for i in range(test_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i + train_size]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_tx.append(i)\n",
    "        col_tx.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_tx.append(doc_vec[j] / doc_len)  # doc_vec[j] / doc_len\n",
    "\n",
    "# tx = sp.csr_matrix((test_size, word_embeddings_dim), dtype=np.float32)\n",
    "tx = sp.csr_matrix((data_tx, (row_tx, col_tx)),\n",
    "                   shape=(test_size, word_embeddings_dim))\n",
    "\n",
    "ty = []\n",
    "for i in range(test_size):\n",
    "    doc_meta = shuffle_doc_name_list[i + train_size]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ty.append(one_hot)\n",
    "ty = np.array(ty)\n",
    "print(ty)\n",
    "\n",
    "# allx: the the feature vectors of both labeled and unlabeled training instances\n",
    "# (a superset of x)\n",
    "# unlabeled training instances -> words\n",
    "\n",
    "word_vectors = np.random.uniform(-0.01, 0.01,\n",
    "                                 (vocab_size, word_embeddings_dim))\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab[i]\n",
    "    if word in word_vector_map:\n",
    "        vector = word_vector_map[word]\n",
    "        word_vectors[i] = vector\n",
    "\n",
    "row_allx = []\n",
    "col_allx = []\n",
    "data_allx = []\n",
    "\n",
    "for i in range(train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i))\n",
    "        col_allx.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_allx.append(doc_vec[j] / doc_len)  # doc_vec[j]/doc_len\n",
    "for i in range(vocab_size):\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i + train_size))\n",
    "        col_allx.append(j)\n",
    "        data_allx.append(word_vectors.item((i, j)))\n",
    "\n",
    "\n",
    "row_allx = np.array(row_allx)\n",
    "col_allx = np.array(col_allx)\n",
    "data_allx = np.array(data_allx)\n",
    "\n",
    "allx = sp.csr_matrix(\n",
    "    (data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embeddings_dim))\n",
    "\n",
    "ally = []\n",
    "for i in range(train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ally.append(one_hot)\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    ally.append(one_hot)\n",
    "\n",
    "ally = np.array(ally)\n",
    "\n",
    "print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n",
    "'''\n",
    "Doc word heterogeneous graph\n",
    "'''\n",
    "\n",
    "# word co-occurence with context windows\n",
    "window_size = 20\n",
    "windows = []\n",
    "\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    length = len(words)\n",
    "    if length <= window_size:\n",
    "        windows.append(words)\n",
    "    else:\n",
    "        # print(length, length - window_size + 1)\n",
    "        for j in range(length - window_size + 1):\n",
    "            window = words[j: j + window_size]\n",
    "            windows.append(window)\n",
    "            # print(window)\n",
    "\n",
    "\n",
    "word_window_freq = {}\n",
    "for window in windows:\n",
    "    appeared = set()\n",
    "    for i in range(len(window)):\n",
    "        if window[i] in appeared:\n",
    "            continue\n",
    "        if window[i] in word_window_freq:\n",
    "            word_window_freq[window[i]] += 1\n",
    "        else:\n",
    "            word_window_freq[window[i]] = 1\n",
    "        appeared.add(window[i])\n",
    "\n",
    "word_pair_count = {}\n",
    "for window in windows:\n",
    "    for i in range(1, len(window)):\n",
    "        for j in range(0, i):\n",
    "            word_i = window[i]\n",
    "            word_i_id = word_id_map[word_i]\n",
    "            word_j = window[j]\n",
    "            word_j_id = word_id_map[word_j]\n",
    "            if word_i_id == word_j_id:\n",
    "                continue\n",
    "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "            # two orders\n",
    "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "\n",
    "row = []\n",
    "col = []\n",
    "weight = []\n",
    "\n",
    "# pmi as weights\n",
    "\n",
    "num_window = len(windows)\n",
    "\n",
    "for key in word_pair_count:\n",
    "    temp = key.split(',')\n",
    "    i = int(temp[0])\n",
    "    j = int(temp[1])\n",
    "    count = word_pair_count[key]\n",
    "    word_freq_i = word_window_freq[vocab[i]]\n",
    "    word_freq_j = word_window_freq[vocab[j]]\n",
    "    pmi = log((1.0 * count / num_window) /\n",
    "              (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n",
    "    if pmi <= 0:\n",
    "        continue\n",
    "    row.append(train_size + i)\n",
    "    col.append(train_size + j)\n",
    "    weight.append(pmi)\n",
    "\n",
    "# word vector cosine similarity as weights\n",
    "\n",
    "'''\n",
    "for i in range(vocab_size):\n",
    "    for j in range(vocab_size):\n",
    "        if vocab[i] in word_vector_map and vocab[j] in word_vector_map:\n",
    "            vector_i = np.array(word_vector_map[vocab[i]])\n",
    "            vector_j = np.array(word_vector_map[vocab[j]])\n",
    "            similarity = 1.0 - cosine(vector_i, vector_j)\n",
    "            if similarity > 0.9:\n",
    "                print(vocab[i], vocab[j], similarity)\n",
    "                row.append(train_size + i)\n",
    "                col.append(train_size + j)\n",
    "                weight.append(similarity)\n",
    "'''\n",
    "# doc word frequency\n",
    "doc_word_freq = {}\n",
    "\n",
    "for doc_id in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[doc_id]\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_id = word_id_map[word]\n",
    "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "        if doc_word_str in doc_word_freq:\n",
    "            doc_word_freq[doc_word_str] += 1\n",
    "        else:\n",
    "            doc_word_freq[doc_word_str] = 1\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_word_set = set()\n",
    "    for word in words:\n",
    "        if word in doc_word_set:\n",
    "            continue\n",
    "        j = word_id_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        freq = doc_word_freq[key]\n",
    "        if i < train_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_size)\n",
    "        col.append(train_size + j)\n",
    "        idf = log(1.0 * len(shuffle_doc_words_list) /\n",
    "                  word_doc_freq[vocab[j]])\n",
    "        weight.append(freq * idf)\n",
    "        doc_word_set.add(word)\n",
    "\n",
    "node_size = train_size + vocab_size + test_size\n",
    "adj = sp.csr_matrix(\n",
    "    (weight, (row, col)), shape=(node_size, node_size))\n",
    "\n",
    "# dump objects\n",
    "f = open(\"data/ind.{}.x\".format(dataset), 'wb')\n",
    "pkl.dump(x, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.y\".format(dataset), 'wb')\n",
    "pkl.dump(y, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.tx\".format(dataset), 'wb')\n",
    "pkl.dump(tx, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.ty\".format(dataset), 'wb')\n",
    "pkl.dump(ty, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.allx\".format(dataset), 'wb')\n",
    "pkl.dump(allx, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.ally\".format(dataset), 'wb')\n",
    "pkl.dump(ally, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"data/ind.{}.adj\".format(dataset), 'wb')\n",
    "pkl.dump(adj, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
